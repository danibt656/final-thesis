{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "787434d2",
   "metadata": {},
   "source": [
    "# PyTorch ResNet + UTKFace\n",
    "\n",
    "Partially based [on this](https://www.kaggle.com/code/gxkok21/resnet50-with-pytorch/notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68adbd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# import skimage\n",
    "from skimage.io import imread\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be24ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder_name = '../data/UTKFace/Images'\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.7\n",
    "IM_WIDTH = IM_HEIGHT = 200\n",
    "\n",
    "dataset_dict = {\n",
    "    'race_id': {\n",
    "        0: 'white', \n",
    "        1: 'black', \n",
    "        2: 'asian', \n",
    "        3: 'indian', \n",
    "        4: 'others'\n",
    "    },\n",
    "    'gender_id': {\n",
    "        0: 'male',\n",
    "        1: 'female'\n",
    "    }\n",
    "}\n",
    "\n",
    "dataset_dict['gender_alias'] = dict((g, i) for i, g in dataset_dict['gender_id'].items())\n",
    "dataset_dict['race_alias'] = dict((g, i) for i, g in dataset_dict['race_id'].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37abcfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(dataset_path, ext='jpg'):\n",
    "    \"\"\"\n",
    "    Used to extract information about our dataset. It does iterate over all images and return a DataFrame with\n",
    "    the data (age, gender and sex) of all files.\n",
    "    \"\"\"\n",
    "    def parse_info_from_file(path):\n",
    "        \"\"\"\n",
    "        Parse information from a single file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            filename = os.path.split(path)[1]\n",
    "            filename = os.path.splitext(filename)[0]\n",
    "            age, gender, race, _ = filename.split('_')\n",
    "\n",
    "            return int(age), dataset_dict['gender_id'][int(gender)], dataset_dict['race_id'][int(race)]\n",
    "        except Exception as ex:\n",
    "            return None, None, None\n",
    "        \n",
    "    files = glob.glob(os.path.join(dataset_path, \"*.%s\" % ext))\n",
    "    \n",
    "    records = []\n",
    "    for file in files:\n",
    "        info = parse_info_from_file(file)\n",
    "        records.append(info)\n",
    "        \n",
    "    df = pd.DataFrame(records)\n",
    "    df['file'] = files\n",
    "    df.columns = ['age', 'gender', 'race', 'file']\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c56bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_dataset(dataset_folder_name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962b2645",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"gender\"].value_counts().plot(kind=\"pie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a1b56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"race\"].value_counts().plot(kind=\"pie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cc1fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, test_indices = train_test_split(df.index, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5828345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8050a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    This is our custom dataset class which will load the images, perform transforms on them,\n",
    "    and load their corresponding labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.images = [os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith(\".jpg\")]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        print(f'#{idx}...', end='')\n",
    "        if idx >= self.df.shape[0]:\n",
    "            idx = self.df.shape[0]-1\n",
    "        img_path = self.df.iloc[idx]['file']\n",
    "#         print(\"img_path:\", img_path)\n",
    "        print('OK')\n",
    "        img = imread(img_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        sample = {\n",
    "            \"image\": img,\n",
    "        }\n",
    "        sample[\"gender\"] = dataset_dict['gender_alias'][self.df.iloc[idx][\"gender\"]]\n",
    "        #sample[\"id\"] = self.df.loc[idx, \"id\"]\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        try:\n",
    "            return self.df.shape[0]\n",
    "        except AttributeError:\n",
    "            return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d47bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pipe = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToPILImage(), # Convert np array to PILImage\n",
    "    \n",
    "    # Resize image to 224 x 224 as required by most vision models\n",
    "    torchvision.transforms.Resize(\n",
    "        size=(224, 224)\n",
    "    ),\n",
    "    \n",
    "    # Convert PIL image to tensor with image values in [0, 1]\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    \n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f12e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset(\n",
    "    df=df,\n",
    "    img_dir=\"../data/UTKFace/Images/\",\n",
    "    transform=transform_pipe\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332217ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training dataset loader will randomly sample from the train samples\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=64,\n",
    "    sampler=torch.utils.data.SubsetRandomSampler(\n",
    "        train_indices\n",
    "    )\n",
    "#     shuffle=True,\n",
    "#     num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a0e05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The testing dataset loader will randomly sample from the test samples\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=64,\n",
    "    sampler=torch.utils.data.SubsetRandomSampler(\n",
    "        test_indices\n",
    "    )\n",
    "#     shuffle=True,\n",
    "#     num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6fbb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"test\": test_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660ec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet50() # WITH pre-trained weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7787b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace final fully connected layer to suite problem\n",
    "model.fc = torch.nn.Sequential(\n",
    "    torch.nn.Linear(\n",
    "        in_features=2048,\n",
    "        out_features=1\n",
    "    ),\n",
    "    torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3d01b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(train_data[0][\"image\"].view(1, 3, 224, 224))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a231b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51843cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "if USE_GPU:\n",
    "    model = model.cuda() # Should be called before instantiating optimizer\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.BCELoss() # For binary classification problem\n",
    "\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    for phase in [\"train\", \"test\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        samples = 0\n",
    "        loss_sum = 0\n",
    "        correct_sum = 0\n",
    "        for j, batch in enumerate(dataloaders[phase]):\n",
    "            X = batch[\"image\"]\n",
    "            genders = batch[\"gender\"]\n",
    "            if USE_GPU:\n",
    "                X = X.cuda()\n",
    "                genders = genders.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                y = model(X)\n",
    "                loss = criterion(\n",
    "                    y, \n",
    "                    genders.view(-1, 1).float()\n",
    "                )\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # We need to multiple by batch size as loss is the mean loss of the samples in the batch\n",
    "                loss_sum += loss.item() * X.shape[0]\n",
    "                samples += X.shape[0]\n",
    "                num_corrects = torch.sum((y >= 0.5).float() == genders.view(-1, 1).float())\n",
    "                correct_sum += num_corrects\n",
    "                \n",
    "                # Print batch statistics every 50 batches\n",
    "                if j % 50 == 49 and phase == \"train\":\n",
    "                    print(\"{}:{} - loss: {}, acc: {}\".format(\n",
    "                        i + 1, \n",
    "                        j + 1, \n",
    "                        float(loss_sum) / float(samples), \n",
    "                        float(correct_sum) / float(samples)\n",
    "                    ))\n",
    "                \n",
    "        # Print epoch statistics\n",
    "        epoch_acc = float(correct_sum) / float(samples)\n",
    "        epoch_loss = float(loss_sum) / float(samples)\n",
    "        print(\"epoch: {} - {} loss: {}, {} acc: {}\".format(i + 1, phase, epoch_loss, phase, epoch_acc))\n",
    "        \n",
    "        # Deep copy the model\n",
    "        if phase == \"test\" and epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_wts, \"resnet50.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct model from saved weights\n",
    "model1 = torchvision.models.resnet50()\n",
    "model1.fc = torch.nn.Sequential(\n",
    "    torch.nn.Linear(\n",
    "        in_features=2048,\n",
    "        out_features=1\n",
    "    ),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "model1.load_state_dict(torch.load(\"resnet50.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1dae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "model1.eval()\n",
    "if USE_GPU:\n",
    "    model1 = model1.cuda()\n",
    "\n",
    "ids_all = []\n",
    "predictions = []\n",
    "\n",
    "for j, batch in enumerate(test_loader1):\n",
    "    X = batch[\"image\"]\n",
    "    ids = batch[\"id\"]\n",
    "    if USE_GPU:\n",
    "        X = X.cuda()\n",
    "    \n",
    "    for _id in ids:\n",
    "        ids_all.append(_id)\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        y_pred = model1(X)\n",
    "        predictions.append((y_pred >= 0.5).float().cpu().numpy())\n",
    "        \n",
    "print(\"Done making predictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a130a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe5f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c050339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0887c1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d1bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfg] *",
   "language": "python",
   "name": "conda-env-tfg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
