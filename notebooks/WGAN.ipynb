{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3150998",
   "metadata": {},
   "source": [
    "# Wasserstein GAN\n",
    "\n",
    "Este modelo implementa la *Generative Adversarial Network* de Arjovsky et al. (2017), una alternativa a la DC-GAN tradicional con la función de loss \"Wasserstein-1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "888882e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8889db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 256\n",
    "img_shape = (3, IMG_SIZE, IMG_SIZE)\n",
    "BATCH_SIZE = 64\n",
    "data_dir = \"../data/PlantVillage/\"\n",
    "\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f7560ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(100, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.shape[0], *img_shape)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.shape[0], -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d767812",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_I = 'Images'\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomSizedCrop(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "]),\n",
    "}\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(data_dir+x,data_transforms[x]) for x in ['train']}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x],\\\n",
    "                                            batch_size=BATCH_SIZE,\\\n",
    "                                            shuffle=shuf,\\\n",
    "                                            num_workers=4)\\\n",
    "              for x,shuf in [('train', True)]}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train']}\n",
    "class_names = [c for c in image_datasets['train'].classes if c != ROOT_I]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "609229ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1984"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloaders['train'])*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4f01e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "\n",
    "# Optimizers\n",
    "# L_R = 0.00005\n",
    "L_R = 0.001\n",
    "optimizer_G = torch.optim.RMSprop(generator.parameters(), lr=L_R)\n",
    "optimizer_D = torch.optim.RMSprop(discriminator.parameters(), lr=L_R)\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a53b3e",
   "metadata": {},
   "source": [
    "## La gente del paper de tomatos entrena la WGAN con 200.000 épocas ;-;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20b419ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/200] [Batch 0/31] [D loss: 0.042334] [G loss: -1.306823]\n",
      "[Epoch 1/200] [Batch 20/31] [D loss: -5488.943848] [G loss: 4020.186768]\n",
      "[Epoch 2/200] [Batch 0/31] [D loss: -1843.124268] [G loss: 15.426870]\n",
      "[Epoch 2/200] [Batch 20/31] [D loss: -1207.727905] [G loss: -397.375793]\n",
      "[Epoch 3/200] [Batch 0/31] [D loss: -243.301697] [G loss: -377.413818]\n",
      "[Epoch 3/200] [Batch 20/31] [D loss: -261.730469] [G loss: -37.577255]\n",
      "[Epoch 4/200] [Batch 0/31] [D loss: -301.850800] [G loss: -19.913982]\n",
      "[Epoch 4/200] [Batch 20/31] [D loss: -189.921082] [G loss: -20.499317]\n",
      "[Epoch 5/200] [Batch 0/31] [D loss: -287.409821] [G loss: -5.769900]\n",
      "[Epoch 5/200] [Batch 20/31] [D loss: -408.996582] [G loss: -27.887245]\n",
      "[Epoch 6/200] [Batch 0/31] [D loss: -291.598328] [G loss: -32.632210]\n",
      "[Epoch 6/200] [Batch 20/31] [D loss: -935.410522] [G loss: 545.153809]\n",
      "[Epoch 7/200] [Batch 0/31] [D loss: -485.065491] [G loss: -5.979647]\n",
      "[Epoch 7/200] [Batch 20/31] [D loss: -473.851654] [G loss: -106.596191]\n",
      "[Epoch 8/200] [Batch 0/31] [D loss: -70.134445] [G loss: -116.311966]\n",
      "[Epoch 8/200] [Batch 20/31] [D loss: -1250.061157] [G loss: 1185.750366]\n",
      "[Epoch 9/200] [Batch 0/31] [D loss: -106.642853] [G loss: 18.727404]\n",
      "[Epoch 9/200] [Batch 20/31] [D loss: -244.464661] [G loss: -102.324402]\n",
      "[Epoch 10/200] [Batch 0/31] [D loss: -12.893723] [G loss: -98.995872]\n",
      "[Epoch 10/200] [Batch 20/31] [D loss: -443.931305] [G loss: 562.545166]\n",
      "[Epoch 11/200] [Batch 0/31] [D loss: -215.867813] [G loss: -150.265106]\n",
      "[Epoch 11/200] [Batch 20/31] [D loss: -284.870728] [G loss: -41.623367]\n",
      "[Epoch 12/200] [Batch 0/31] [D loss: -205.926498] [G loss: -55.345558]\n",
      "[Epoch 12/200] [Batch 20/31] [D loss: -1080.477539] [G loss: 2102.966064]\n",
      "[Epoch 13/200] [Batch 0/31] [D loss: -268.977356] [G loss: 636.551880]\n",
      "[Epoch 13/200] [Batch 20/31] [D loss: -485.344513] [G loss: 361.846893]\n",
      "[Epoch 14/200] [Batch 0/31] [D loss: -162.222839] [G loss: -713.555298]\n",
      "[Epoch 14/200] [Batch 20/31] [D loss: -889.322998] [G loss: -376.926697]\n",
      "[Epoch 15/200] [Batch 0/31] [D loss: -88.084122] [G loss: 65.558311]\n",
      "[Epoch 15/200] [Batch 20/31] [D loss: -924.416687] [G loss: 1150.330933]\n",
      "[Epoch 16/200] [Batch 0/31] [D loss: -377.852631] [G loss: 572.666260]\n",
      "[Epoch 16/200] [Batch 20/31] [D loss: -517.591919] [G loss: -132.684189]\n",
      "[Epoch 17/200] [Batch 0/31] [D loss: -153.954620] [G loss: -37.919773]\n",
      "[Epoch 17/200] [Batch 20/31] [D loss: -302.119751] [G loss: 469.546021]\n",
      "[Epoch 18/200] [Batch 0/31] [D loss: -250.306107] [G loss: 75.324318]\n",
      "[Epoch 18/200] [Batch 20/31] [D loss: -264.028076] [G loss: 56.140717]\n",
      "[Epoch 19/200] [Batch 0/31] [D loss: -87.676918] [G loss: 29.451653]\n",
      "[Epoch 19/200] [Batch 20/31] [D loss: -157.534286] [G loss: 314.935486]\n",
      "[Epoch 20/200] [Batch 0/31] [D loss: -421.171173] [G loss: -97.526459]\n",
      "[Epoch 20/200] [Batch 20/31] [D loss: -429.600281] [G loss: -285.059448]\n",
      "[Epoch 21/200] [Batch 0/31] [D loss: -299.409973] [G loss: -501.820740]\n",
      "[Epoch 21/200] [Batch 20/31] [D loss: -186.969101] [G loss: -105.413620]\n",
      "[Epoch 22/200] [Batch 0/31] [D loss: -120.757637] [G loss: -18.598904]\n",
      "[Epoch 22/200] [Batch 20/31] [D loss: -223.915375] [G loss: -41.330345]\n",
      "[Epoch 23/200] [Batch 0/31] [D loss: -117.403969] [G loss: -28.664085]\n",
      "[Epoch 23/200] [Batch 20/31] [D loss: -343.467285] [G loss: 248.135117]\n",
      "[Epoch 24/200] [Batch 0/31] [D loss: -346.982422] [G loss: 120.485909]\n",
      "[Epoch 24/200] [Batch 20/31] [D loss: -362.446869] [G loss: 180.260101]\n",
      "[Epoch 25/200] [Batch 0/31] [D loss: -193.864182] [G loss: 22.260065]\n",
      "[Epoch 25/200] [Batch 20/31] [D loss: -272.677856] [G loss: 51.209549]\n",
      "[Epoch 26/200] [Batch 0/31] [D loss: -246.821960] [G loss: 8.524251]\n",
      "[Epoch 26/200] [Batch 20/31] [D loss: -297.417419] [G loss: 101.506439]\n",
      "[Epoch 27/200] [Batch 0/31] [D loss: -292.270233] [G loss: -90.992668]\n",
      "[Epoch 27/200] [Batch 20/31] [D loss: -273.818909] [G loss: -179.272858]\n",
      "[Epoch 28/200] [Batch 0/31] [D loss: -246.066833] [G loss: -115.544449]\n",
      "[Epoch 28/200] [Batch 20/31] [D loss: -314.642456] [G loss: 233.974945]\n",
      "[Epoch 29/200] [Batch 0/31] [D loss: -274.581024] [G loss: -45.797451]\n",
      "[Epoch 29/200] [Batch 20/31] [D loss: -352.804993] [G loss: -123.029556]\n",
      "[Epoch 30/200] [Batch 0/31] [D loss: -257.184814] [G loss: -57.514572]\n",
      "[Epoch 30/200] [Batch 20/31] [D loss: -208.951553] [G loss: 163.580643]\n",
      "[Epoch 31/200] [Batch 0/31] [D loss: -159.100403] [G loss: -240.613312]\n",
      "[Epoch 31/200] [Batch 20/31] [D loss: -225.447021] [G loss: 326.942413]\n",
      "[Epoch 32/200] [Batch 0/31] [D loss: -278.058594] [G loss: -335.856476]\n",
      "[Epoch 32/200] [Batch 20/31] [D loss: -356.415619] [G loss: 453.154388]\n",
      "[Epoch 33/200] [Batch 0/31] [D loss: -261.737335] [G loss: -82.532761]\n",
      "[Epoch 33/200] [Batch 20/31] [D loss: -319.004059] [G loss: -259.915039]\n",
      "[Epoch 34/200] [Batch 0/31] [D loss: -398.719757] [G loss: 500.370605]\n",
      "[Epoch 34/200] [Batch 20/31] [D loss: -349.232239] [G loss: -243.791229]\n",
      "[Epoch 35/200] [Batch 0/31] [D loss: -280.043427] [G loss: 42.518425]\n",
      "[Epoch 35/200] [Batch 20/31] [D loss: -368.479797] [G loss: 300.122986]\n",
      "[Epoch 36/200] [Batch 0/31] [D loss: -381.542877] [G loss: -312.779053]\n",
      "[Epoch 36/200] [Batch 20/31] [D loss: -322.932770] [G loss: 353.475830]\n",
      "[Epoch 37/200] [Batch 0/31] [D loss: -286.189423] [G loss: -242.087097]\n",
      "[Epoch 37/200] [Batch 20/31] [D loss: -249.615021] [G loss: 261.813354]\n",
      "[Epoch 38/200] [Batch 0/31] [D loss: -251.159653] [G loss: -76.334351]\n",
      "[Epoch 38/200] [Batch 20/31] [D loss: -316.055511] [G loss: -12.975062]\n",
      "[Epoch 39/200] [Batch 0/31] [D loss: -272.761719] [G loss: 300.825104]\n",
      "[Epoch 39/200] [Batch 20/31] [D loss: -350.005096] [G loss: -103.911316]\n",
      "[Epoch 40/200] [Batch 0/31] [D loss: -213.381027] [G loss: -92.075333]\n",
      "[Epoch 40/200] [Batch 20/31] [D loss: -343.558899] [G loss: 328.365356]\n",
      "[Epoch 41/200] [Batch 0/31] [D loss: -277.882202] [G loss: -172.354736]\n",
      "[Epoch 41/200] [Batch 20/31] [D loss: -363.128632] [G loss: 137.388702]\n",
      "[Epoch 42/200] [Batch 0/31] [D loss: -289.014832] [G loss: 125.955246]\n",
      "[Epoch 42/200] [Batch 20/31] [D loss: -308.353119] [G loss: -30.531475]\n",
      "[Epoch 43/200] [Batch 0/31] [D loss: -284.397156] [G loss: -31.815170]\n",
      "[Epoch 43/200] [Batch 20/31] [D loss: -275.688629] [G loss: 279.837738]\n",
      "[Epoch 44/200] [Batch 0/31] [D loss: -260.855957] [G loss: -146.581467]\n",
      "[Epoch 44/200] [Batch 20/31] [D loss: -304.286682] [G loss: -34.930119]\n",
      "[Epoch 45/200] [Batch 0/31] [D loss: -177.274551] [G loss: 119.708405]\n",
      "[Epoch 45/200] [Batch 20/31] [D loss: -225.590942] [G loss: -53.731979]\n",
      "[Epoch 46/200] [Batch 0/31] [D loss: -205.860718] [G loss: 245.053329]\n",
      "[Epoch 46/200] [Batch 20/31] [D loss: -237.941620] [G loss: -112.658073]\n",
      "[Epoch 47/200] [Batch 0/31] [D loss: -122.955597] [G loss: 53.819012]\n",
      "[Epoch 47/200] [Batch 20/31] [D loss: -248.889633] [G loss: 216.421265]\n",
      "[Epoch 48/200] [Batch 0/31] [D loss: -149.667847] [G loss: -86.429092]\n",
      "[Epoch 48/200] [Batch 20/31] [D loss: -260.714752] [G loss: -124.687874]\n",
      "[Epoch 49/200] [Batch 0/31] [D loss: -203.869141] [G loss: 220.648865]\n",
      "[Epoch 49/200] [Batch 20/31] [D loss: -242.234253] [G loss: -86.343109]\n",
      "[Epoch 50/200] [Batch 0/31] [D loss: -268.632263] [G loss: -137.244965]\n",
      "[Epoch 50/200] [Batch 20/31] [D loss: -277.760132] [G loss: 298.011536]\n",
      "[Epoch 51/200] [Batch 0/31] [D loss: -272.697876] [G loss: -87.095398]\n",
      "[Epoch 51/200] [Batch 20/31] [D loss: -361.777222] [G loss: 39.475155]\n",
      "[Epoch 52/200] [Batch 0/31] [D loss: -267.287537] [G loss: -70.654388]\n",
      "[Epoch 52/200] [Batch 20/31] [D loss: -311.125122] [G loss: 283.598328]\n",
      "[Epoch 53/200] [Batch 0/31] [D loss: -259.397522] [G loss: -138.508057]\n",
      "[Epoch 53/200] [Batch 20/31] [D loss: -267.137573] [G loss: 3.789974]\n",
      "[Epoch 54/200] [Batch 0/31] [D loss: -204.121719] [G loss: 63.304321]\n",
      "[Epoch 54/200] [Batch 20/31] [D loss: -277.190491] [G loss: -82.544144]\n",
      "[Epoch 55/200] [Batch 0/31] [D loss: -203.463257] [G loss: 138.257599]\n",
      "[Epoch 55/200] [Batch 20/31] [D loss: -327.272400] [G loss: -36.333164]\n",
      "[Epoch 56/200] [Batch 0/31] [D loss: -198.946716] [G loss: 42.645454]\n",
      "[Epoch 56/200] [Batch 20/31] [D loss: -346.940216] [G loss: 129.371750]\n",
      "[Epoch 57/200] [Batch 0/31] [D loss: -159.381165] [G loss: 16.361742]\n",
      "[Epoch 57/200] [Batch 20/31] [D loss: -264.388519] [G loss: -92.124283]\n",
      "[Epoch 58/200] [Batch 0/31] [D loss: -239.841553] [G loss: 206.396179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 58/200] [Batch 20/31] [D loss: -229.516129] [G loss: -149.181442]\n",
      "[Epoch 59/200] [Batch 0/31] [D loss: -186.472595] [G loss: 209.947876]\n",
      "[Epoch 59/200] [Batch 20/31] [D loss: -298.932617] [G loss: -42.057163]\n",
      "[Epoch 60/200] [Batch 0/31] [D loss: -211.534332] [G loss: -86.537605]\n",
      "[Epoch 60/200] [Batch 20/31] [D loss: -307.360016] [G loss: 221.319046]\n",
      "[Epoch 61/200] [Batch 0/31] [D loss: -259.135773] [G loss: -85.678680]\n",
      "[Epoch 61/200] [Batch 20/31] [D loss: -293.285187] [G loss: -32.541977]\n",
      "[Epoch 62/200] [Batch 0/31] [D loss: -261.860352] [G loss: 272.726288]\n",
      "[Epoch 62/200] [Batch 20/31] [D loss: -247.019653] [G loss: -205.000153]\n",
      "[Epoch 63/200] [Batch 0/31] [D loss: -300.527405] [G loss: 262.610474]\n",
      "[Epoch 63/200] [Batch 20/31] [D loss: -279.607422] [G loss: -74.857162]\n",
      "[Epoch 64/200] [Batch 0/31] [D loss: -200.567123] [G loss: -1.149960]\n",
      "[Epoch 64/200] [Batch 20/31] [D loss: -306.023499] [G loss: 100.231300]\n",
      "[Epoch 65/200] [Batch 0/31] [D loss: -264.123932] [G loss: -81.190308]\n",
      "[Epoch 65/200] [Batch 20/31] [D loss: -219.830093] [G loss: -115.951729]\n",
      "[Epoch 66/200] [Batch 0/31] [D loss: -256.989838] [G loss: 310.710968]\n",
      "[Epoch 66/200] [Batch 20/31] [D loss: -323.511230] [G loss: -167.297806]\n",
      "[Epoch 67/200] [Batch 0/31] [D loss: -248.372452] [G loss: 230.355316]\n",
      "[Epoch 67/200] [Batch 20/31] [D loss: -270.234436] [G loss: -66.067299]\n",
      "[Epoch 68/200] [Batch 0/31] [D loss: -246.295532] [G loss: 132.695679]\n",
      "[Epoch 68/200] [Batch 20/31] [D loss: -281.583740] [G loss: -57.857059]\n",
      "[Epoch 69/200] [Batch 0/31] [D loss: -246.558624] [G loss: 166.604462]\n",
      "[Epoch 69/200] [Batch 20/31] [D loss: -222.629272] [G loss: -1.148854]\n",
      "[Epoch 70/200] [Batch 0/31] [D loss: -205.181808] [G loss: 33.428680]\n",
      "[Epoch 70/200] [Batch 20/31] [D loss: -277.140839] [G loss: -58.536213]\n",
      "[Epoch 71/200] [Batch 0/31] [D loss: -188.001953] [G loss: -60.624557]\n",
      "[Epoch 71/200] [Batch 20/31] [D loss: -309.032837] [G loss: 30.571430]\n",
      "[Epoch 72/200] [Batch 0/31] [D loss: -235.744095] [G loss: 308.953217]\n",
      "[Epoch 72/200] [Batch 20/31] [D loss: -368.299316] [G loss: -119.113243]\n",
      "[Epoch 73/200] [Batch 0/31] [D loss: -248.952820] [G loss: 254.523972]\n",
      "[Epoch 73/200] [Batch 20/31] [D loss: -258.648010] [G loss: -92.204391]\n",
      "[Epoch 74/200] [Batch 0/31] [D loss: -225.322998] [G loss: 43.832298]\n",
      "[Epoch 74/200] [Batch 20/31] [D loss: -258.822144] [G loss: 50.945141]\n",
      "[Epoch 75/200] [Batch 0/31] [D loss: -209.894470] [G loss: -148.223618]\n",
      "[Epoch 75/200] [Batch 20/31] [D loss: -300.384430] [G loss: 250.524628]\n",
      "[Epoch 76/200] [Batch 0/31] [D loss: -180.752655] [G loss: -39.681927]\n",
      "[Epoch 76/200] [Batch 20/31] [D loss: -248.261383] [G loss: 66.367668]\n",
      "[Epoch 77/200] [Batch 0/31] [D loss: -232.622772] [G loss: 26.532812]\n",
      "[Epoch 77/200] [Batch 20/31] [D loss: -219.061478] [G loss: 156.644516]\n",
      "[Epoch 78/200] [Batch 0/31] [D loss: -207.094574] [G loss: -198.388626]\n",
      "[Epoch 78/200] [Batch 20/31] [D loss: -211.209534] [G loss: 155.812622]\n",
      "[Epoch 79/200] [Batch 0/31] [D loss: -215.465668] [G loss: -151.554169]\n",
      "[Epoch 79/200] [Batch 20/31] [D loss: -205.823929] [G loss: 23.329729]\n",
      "[Epoch 80/200] [Batch 0/31] [D loss: -187.911255] [G loss: 173.582962]\n",
      "[Epoch 80/200] [Batch 20/31] [D loss: -300.829041] [G loss: 58.238182]\n",
      "[Epoch 81/200] [Batch 0/31] [D loss: -242.159698] [G loss: -68.834724]\n",
      "[Epoch 81/200] [Batch 20/31] [D loss: -251.931305] [G loss: 75.619080]\n",
      "[Epoch 82/200] [Batch 0/31] [D loss: -213.238907] [G loss: -125.648026]\n",
      "[Epoch 82/200] [Batch 20/31] [D loss: -238.293503] [G loss: 103.058670]\n",
      "[Epoch 83/200] [Batch 0/31] [D loss: -257.682800] [G loss: 198.544922]\n",
      "[Epoch 83/200] [Batch 20/31] [D loss: -262.390259] [G loss: -126.100380]\n",
      "[Epoch 84/200] [Batch 0/31] [D loss: -252.158630] [G loss: 155.154861]\n",
      "[Epoch 84/200] [Batch 20/31] [D loss: -237.014908] [G loss: 183.826096]\n",
      "[Epoch 85/200] [Batch 0/31] [D loss: -287.063354] [G loss: -117.256271]\n",
      "[Epoch 85/200] [Batch 20/31] [D loss: -293.554443] [G loss: -16.749672]\n",
      "[Epoch 86/200] [Batch 0/31] [D loss: -217.221649] [G loss: 157.943787]\n",
      "[Epoch 86/200] [Batch 20/31] [D loss: -227.582336] [G loss: -52.217384]\n",
      "[Epoch 87/200] [Batch 0/31] [D loss: -182.830261] [G loss: -160.415283]\n",
      "[Epoch 87/200] [Batch 20/31] [D loss: -301.065674] [G loss: 396.499573]\n",
      "[Epoch 88/200] [Batch 0/31] [D loss: -219.478485] [G loss: -149.192841]\n",
      "[Epoch 88/200] [Batch 20/31] [D loss: -206.219727] [G loss: -104.087296]\n",
      "[Epoch 89/200] [Batch 0/31] [D loss: -186.851486] [G loss: 207.807648]\n",
      "[Epoch 89/200] [Batch 20/31] [D loss: -277.137909] [G loss: -110.053589]\n",
      "[Epoch 90/200] [Batch 0/31] [D loss: -179.699707] [G loss: 174.464966]\n",
      "[Epoch 90/200] [Batch 20/31] [D loss: -207.938141] [G loss: 173.517822]\n",
      "[Epoch 91/200] [Batch 0/31] [D loss: -181.029175] [G loss: -138.094131]\n",
      "[Epoch 91/200] [Batch 20/31] [D loss: -213.620575] [G loss: -40.947678]\n",
      "[Epoch 92/200] [Batch 0/31] [D loss: -213.123047] [G loss: 242.995697]\n",
      "[Epoch 92/200] [Batch 20/31] [D loss: -230.329025] [G loss: -87.029221]\n",
      "[Epoch 93/200] [Batch 0/31] [D loss: -185.610077] [G loss: 231.445572]\n",
      "[Epoch 93/200] [Batch 20/31] [D loss: -268.936401] [G loss: -211.736328]\n",
      "[Epoch 94/200] [Batch 0/31] [D loss: -150.769348] [G loss: 355.581848]\n",
      "[Epoch 94/200] [Batch 20/31] [D loss: -254.940720] [G loss: -252.165573]\n",
      "[Epoch 95/200] [Batch 0/31] [D loss: -210.094452] [G loss: 398.844360]\n",
      "[Epoch 95/200] [Batch 20/31] [D loss: -241.636993] [G loss: -252.317596]\n",
      "[Epoch 96/200] [Batch 0/31] [D loss: -218.321686] [G loss: 115.529373]\n",
      "[Epoch 96/200] [Batch 20/31] [D loss: -240.813217] [G loss: -21.506620]\n",
      "[Epoch 97/200] [Batch 0/31] [D loss: -203.059128] [G loss: 131.642609]\n",
      "[Epoch 97/200] [Batch 20/31] [D loss: -220.439255] [G loss: -42.555641]\n",
      "[Epoch 98/200] [Batch 0/31] [D loss: -196.698318] [G loss: -22.896688]\n",
      "[Epoch 98/200] [Batch 20/31] [D loss: -235.978165] [G loss: -64.327332]\n",
      "[Epoch 99/200] [Batch 0/31] [D loss: -205.324585] [G loss: 187.989319]\n",
      "[Epoch 99/200] [Batch 20/31] [D loss: -254.524933] [G loss: -19.057621]\n",
      "[Epoch 100/200] [Batch 0/31] [D loss: -194.505035] [G loss: -7.745416]\n",
      "[Epoch 100/200] [Batch 20/31] [D loss: -253.068558] [G loss: 82.721710]\n",
      "[Epoch 101/200] [Batch 0/31] [D loss: -208.045380] [G loss: -7.608813]\n",
      "[Epoch 101/200] [Batch 20/31] [D loss: -206.352707] [G loss: -6.171829]\n",
      "[Epoch 102/200] [Batch 0/31] [D loss: -218.851959] [G loss: 50.425476]\n",
      "[Epoch 102/200] [Batch 20/31] [D loss: -202.897293] [G loss: 19.342514]\n",
      "[Epoch 103/200] [Batch 0/31] [D loss: -176.205841] [G loss: 10.985798]\n",
      "[Epoch 103/200] [Batch 20/31] [D loss: -265.862091] [G loss: -1.704941]\n",
      "[Epoch 104/200] [Batch 0/31] [D loss: -248.869141] [G loss: 203.741196]\n",
      "[Epoch 104/200] [Batch 20/31] [D loss: -216.799591] [G loss: -45.516380]\n",
      "[Epoch 105/200] [Batch 0/31] [D loss: -183.923737] [G loss: -20.103622]\n",
      "[Epoch 105/200] [Batch 20/31] [D loss: -234.213181] [G loss: 67.120979]\n",
      "[Epoch 106/200] [Batch 0/31] [D loss: -213.678970] [G loss: -46.010910]\n",
      "[Epoch 106/200] [Batch 20/31] [D loss: -247.155518] [G loss: 82.287613]\n",
      "[Epoch 107/200] [Batch 0/31] [D loss: -208.097107] [G loss: -38.540409]\n",
      "[Epoch 107/200] [Batch 20/31] [D loss: -269.770355] [G loss: -76.827431]\n",
      "[Epoch 108/200] [Batch 0/31] [D loss: -241.410614] [G loss: 211.101227]\n",
      "[Epoch 108/200] [Batch 20/31] [D loss: -226.084137] [G loss: 3.137656]\n",
      "[Epoch 109/200] [Batch 0/31] [D loss: -182.813232] [G loss: 107.595528]\n",
      "[Epoch 109/200] [Batch 20/31] [D loss: -239.594040] [G loss: -63.790970]\n",
      "[Epoch 110/200] [Batch 0/31] [D loss: -175.798325] [G loss: 118.716682]\n",
      "[Epoch 110/200] [Batch 20/31] [D loss: -268.104004] [G loss: -279.978455]\n",
      "[Epoch 111/200] [Batch 0/31] [D loss: -201.315674] [G loss: 257.093018]\n",
      "[Epoch 111/200] [Batch 20/31] [D loss: -239.368408] [G loss: -49.682610]\n",
      "[Epoch 112/200] [Batch 0/31] [D loss: -237.974243] [G loss: 150.625626]\n",
      "[Epoch 112/200] [Batch 20/31] [D loss: -221.029480] [G loss: -46.948647]\n",
      "[Epoch 113/200] [Batch 0/31] [D loss: -184.416672] [G loss: -27.803764]\n",
      "[Epoch 113/200] [Batch 20/31] [D loss: -234.063782] [G loss: 100.988396]\n",
      "[Epoch 114/200] [Batch 0/31] [D loss: -226.813797] [G loss: -92.170479]\n",
      "[Epoch 114/200] [Batch 20/31] [D loss: -257.815674] [G loss: 320.094910]\n",
      "[Epoch 115/200] [Batch 0/31] [D loss: -233.463699] [G loss: -138.624756]\n",
      "[Epoch 115/200] [Batch 20/31] [D loss: -219.204086] [G loss: -3.228868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 116/200] [Batch 0/31] [D loss: -259.978455] [G loss: -99.804092]\n",
      "[Epoch 116/200] [Batch 20/31] [D loss: -234.933212] [G loss: 173.553589]\n",
      "[Epoch 117/200] [Batch 0/31] [D loss: -194.081726] [G loss: 105.665070]\n",
      "[Epoch 117/200] [Batch 20/31] [D loss: -238.716629] [G loss: -42.241142]\n",
      "[Epoch 118/200] [Batch 0/31] [D loss: -174.560089] [G loss: -95.945045]\n",
      "[Epoch 118/200] [Batch 20/31] [D loss: -239.171417] [G loss: 278.692566]\n",
      "[Epoch 119/200] [Batch 0/31] [D loss: -196.025116] [G loss: -186.148834]\n",
      "[Epoch 119/200] [Batch 20/31] [D loss: -224.450714] [G loss: 244.388123]\n",
      "[Epoch 120/200] [Batch 0/31] [D loss: -197.063202] [G loss: -193.758148]\n",
      "[Epoch 120/200] [Batch 20/31] [D loss: -262.878418] [G loss: 141.733170]\n",
      "[Epoch 121/200] [Batch 0/31] [D loss: -197.891113] [G loss: -44.165512]\n",
      "[Epoch 121/200] [Batch 20/31] [D loss: -244.635422] [G loss: 157.953384]\n",
      "[Epoch 122/200] [Batch 0/31] [D loss: -204.541534] [G loss: -73.650826]\n",
      "[Epoch 122/200] [Batch 20/31] [D loss: -249.296661] [G loss: 198.106903]\n",
      "[Epoch 123/200] [Batch 0/31] [D loss: -228.492371] [G loss: -137.017578]\n",
      "[Epoch 123/200] [Batch 20/31] [D loss: -287.274353] [G loss: -2.777649]\n",
      "[Epoch 124/200] [Batch 0/31] [D loss: -189.140167] [G loss: 30.335978]\n",
      "[Epoch 124/200] [Batch 20/31] [D loss: -241.667313] [G loss: 134.504303]\n",
      "[Epoch 125/200] [Batch 0/31] [D loss: -170.798828] [G loss: -6.972071]\n",
      "[Epoch 125/200] [Batch 20/31] [D loss: -197.659302] [G loss: -38.335121]\n",
      "[Epoch 126/200] [Batch 0/31] [D loss: -197.803238] [G loss: -1.008523]\n",
      "[Epoch 126/200] [Batch 20/31] [D loss: -248.566040] [G loss: 78.983925]\n",
      "[Epoch 127/200] [Batch 0/31] [D loss: -198.380035] [G loss: -58.425018]\n",
      "[Epoch 127/200] [Batch 20/31] [D loss: -236.748917] [G loss: 113.945862]\n",
      "[Epoch 128/200] [Batch 0/31] [D loss: -183.240463] [G loss: -119.631287]\n",
      "[Epoch 128/200] [Batch 20/31] [D loss: -276.024200] [G loss: 176.761551]\n",
      "[Epoch 129/200] [Batch 0/31] [D loss: -201.975342] [G loss: -159.045853]\n",
      "[Epoch 129/200] [Batch 20/31] [D loss: -272.706604] [G loss: 299.831665]\n",
      "[Epoch 130/200] [Batch 0/31] [D loss: -283.781372] [G loss: -130.488205]\n",
      "[Epoch 130/200] [Batch 20/31] [D loss: -147.911896] [G loss: -133.568237]\n",
      "[Epoch 131/200] [Batch 0/31] [D loss: -220.784912] [G loss: 303.466553]\n",
      "[Epoch 131/200] [Batch 20/31] [D loss: -215.088364] [G loss: -138.624176]\n",
      "[Epoch 132/200] [Batch 0/31] [D loss: -186.752686] [G loss: 294.726929]\n",
      "[Epoch 132/200] [Batch 20/31] [D loss: -245.105774] [G loss: -227.442795]\n",
      "[Epoch 133/200] [Batch 0/31] [D loss: -225.158325] [G loss: 9.339479]\n",
      "[Epoch 133/200] [Batch 20/31] [D loss: -229.244980] [G loss: 247.950775]\n",
      "[Epoch 134/200] [Batch 0/31] [D loss: -171.037247] [G loss: -125.161163]\n",
      "[Epoch 134/200] [Batch 20/31] [D loss: -248.291077] [G loss: -95.931732]\n",
      "[Epoch 135/200] [Batch 0/31] [D loss: -185.439209] [G loss: 283.580383]\n",
      "[Epoch 135/200] [Batch 20/31] [D loss: -242.732239] [G loss: -34.005825]\n",
      "[Epoch 136/200] [Batch 0/31] [D loss: -178.727676] [G loss: -179.587784]\n",
      "[Epoch 136/200] [Batch 20/31] [D loss: -230.455551] [G loss: 319.122314]\n",
      "[Epoch 137/200] [Batch 0/31] [D loss: -174.432251] [G loss: -66.503983]\n",
      "[Epoch 137/200] [Batch 20/31] [D loss: -243.538879] [G loss: -143.781204]\n",
      "[Epoch 138/200] [Batch 0/31] [D loss: -159.744476] [G loss: 151.638199]\n",
      "[Epoch 138/200] [Batch 20/31] [D loss: -175.601715] [G loss: -51.293129]\n",
      "[Epoch 139/200] [Batch 0/31] [D loss: -152.776016] [G loss: 117.938324]\n",
      "[Epoch 139/200] [Batch 20/31] [D loss: -208.779617] [G loss: -105.121002]\n",
      "[Epoch 140/200] [Batch 0/31] [D loss: -209.880432] [G loss: -165.528824]\n",
      "[Epoch 140/200] [Batch 20/31] [D loss: -181.542267] [G loss: 208.358032]\n",
      "[Epoch 141/200] [Batch 0/31] [D loss: -195.743637] [G loss: 113.754761]\n",
      "[Epoch 141/200] [Batch 20/31] [D loss: -176.661224] [G loss: -92.725334]\n",
      "[Epoch 142/200] [Batch 0/31] [D loss: -173.198975] [G loss: 217.697906]\n",
      "[Epoch 142/200] [Batch 20/31] [D loss: -200.694824] [G loss: -174.707306]\n",
      "[Epoch 143/200] [Batch 0/31] [D loss: -186.900772] [G loss: 200.147797]\n",
      "[Epoch 143/200] [Batch 20/31] [D loss: -205.015854] [G loss: -81.081993]\n",
      "[Epoch 144/200] [Batch 0/31] [D loss: -160.478455] [G loss: -102.451675]\n",
      "[Epoch 144/200] [Batch 20/31] [D loss: -245.204926] [G loss: 342.138184]\n",
      "[Epoch 145/200] [Batch 0/31] [D loss: -195.116211] [G loss: -240.167648]\n",
      "[Epoch 145/200] [Batch 20/31] [D loss: -243.338959] [G loss: -38.388126]\n",
      "[Epoch 146/200] [Batch 0/31] [D loss: -146.217819] [G loss: 139.404388]\n",
      "[Epoch 146/200] [Batch 20/31] [D loss: -238.814819] [G loss: 137.269806]\n",
      "[Epoch 147/200] [Batch 0/31] [D loss: -169.083633] [G loss: -103.727417]\n",
      "[Epoch 147/200] [Batch 20/31] [D loss: -218.449448] [G loss: 231.050201]\n",
      "[Epoch 148/200] [Batch 0/31] [D loss: -159.434280] [G loss: -114.021301]\n",
      "[Epoch 148/200] [Batch 20/31] [D loss: -194.071411] [G loss: 16.833179]\n",
      "[Epoch 149/200] [Batch 0/31] [D loss: -167.139694] [G loss: 59.194588]\n",
      "[Epoch 149/200] [Batch 20/31] [D loss: -217.442062] [G loss: 221.216080]\n",
      "[Epoch 150/200] [Batch 0/31] [D loss: -189.530243] [G loss: -251.540405]\n",
      "[Epoch 150/200] [Batch 20/31] [D loss: -258.243103] [G loss: -78.695206]\n",
      "[Epoch 151/200] [Batch 0/31] [D loss: -185.273727] [G loss: 342.758972]\n",
      "[Epoch 151/200] [Batch 20/31] [D loss: -233.659088] [G loss: -65.209274]\n",
      "[Epoch 152/200] [Batch 0/31] [D loss: -165.304855] [G loss: -43.649319]\n",
      "[Epoch 152/200] [Batch 20/31] [D loss: -218.729248] [G loss: -26.735779]\n",
      "[Epoch 153/200] [Batch 0/31] [D loss: -180.443329] [G loss: 108.610550]\n",
      "[Epoch 153/200] [Batch 20/31] [D loss: -183.534836] [G loss: 73.845901]\n",
      "[Epoch 154/200] [Batch 0/31] [D loss: -158.782562] [G loss: 30.460480]\n",
      "[Epoch 154/200] [Batch 20/31] [D loss: -207.287109] [G loss: -101.324188]\n",
      "[Epoch 155/200] [Batch 0/31] [D loss: -165.722183] [G loss: 302.200958]\n",
      "[Epoch 155/200] [Batch 20/31] [D loss: -217.519638] [G loss: -99.256905]\n",
      "[Epoch 156/200] [Batch 0/31] [D loss: -138.889465] [G loss: -18.289381]\n",
      "[Epoch 156/200] [Batch 20/31] [D loss: -236.268646] [G loss: -13.490883]\n",
      "[Epoch 157/200] [Batch 0/31] [D loss: -161.439484] [G loss: -83.990639]\n",
      "[Epoch 157/200] [Batch 20/31] [D loss: -217.619644] [G loss: 319.456512]\n",
      "[Epoch 158/200] [Batch 0/31] [D loss: -177.848801] [G loss: -233.135025]\n",
      "[Epoch 158/200] [Batch 20/31] [D loss: -228.490326] [G loss: 289.408203]\n",
      "[Epoch 159/200] [Batch 0/31] [D loss: -214.193970] [G loss: -111.448074]\n",
      "[Epoch 159/200] [Batch 20/31] [D loss: -267.347107] [G loss: 229.454010]\n",
      "[Epoch 160/200] [Batch 0/31] [D loss: -163.904877] [G loss: -112.690536]\n",
      "[Epoch 160/200] [Batch 20/31] [D loss: -243.414444] [G loss: 195.965317]\n",
      "[Epoch 161/200] [Batch 0/31] [D loss: -139.838867] [G loss: -188.461609]\n",
      "[Epoch 161/200] [Batch 20/31] [D loss: -269.397003] [G loss: 295.284760]\n",
      "[Epoch 162/200] [Batch 0/31] [D loss: -174.670593] [G loss: -267.819153]\n",
      "[Epoch 162/200] [Batch 20/31] [D loss: -273.260742] [G loss: 300.224609]\n",
      "[Epoch 163/200] [Batch 0/31] [D loss: -187.267578] [G loss: -143.553757]\n",
      "[Epoch 163/200] [Batch 20/31] [D loss: -199.651245] [G loss: 33.980335]\n",
      "[Epoch 164/200] [Batch 0/31] [D loss: -167.761581] [G loss: 151.545837]\n",
      "[Epoch 164/200] [Batch 20/31] [D loss: -214.419281] [G loss: -88.074860]\n",
      "[Epoch 165/200] [Batch 0/31] [D loss: -222.297852] [G loss: 252.097580]\n",
      "[Epoch 165/200] [Batch 20/31] [D loss: -225.921371] [G loss: -73.622116]\n",
      "[Epoch 166/200] [Batch 0/31] [D loss: -175.792191] [G loss: -53.023247]\n",
      "[Epoch 166/200] [Batch 20/31] [D loss: -236.598007] [G loss: 36.603729]\n",
      "[Epoch 167/200] [Batch 0/31] [D loss: -162.771866] [G loss: 184.327362]\n",
      "[Epoch 167/200] [Batch 20/31] [D loss: -219.791473] [G loss: -70.309837]\n",
      "[Epoch 168/200] [Batch 0/31] [D loss: -113.119148] [G loss: 55.161186]\n",
      "[Epoch 168/200] [Batch 20/31] [D loss: -226.026611] [G loss: 94.999146]\n",
      "[Epoch 169/200] [Batch 0/31] [D loss: -194.507141] [G loss: 42.111462]\n",
      "[Epoch 169/200] [Batch 20/31] [D loss: -223.013779] [G loss: -156.219452]\n",
      "[Epoch 170/200] [Batch 0/31] [D loss: -135.751450] [G loss: 208.935791]\n",
      "[Epoch 170/200] [Batch 20/31] [D loss: -276.854034] [G loss: -68.610199]\n",
      "[Epoch 171/200] [Batch 0/31] [D loss: -132.904831] [G loss: 23.029045]\n",
      "[Epoch 171/200] [Batch 20/31] [D loss: -217.135910] [G loss: 55.718147]\n",
      "[Epoch 172/200] [Batch 0/31] [D loss: -178.636002] [G loss: -91.363510]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 172/200] [Batch 20/31] [D loss: -178.072357] [G loss: -54.578377]\n",
      "[Epoch 173/200] [Batch 0/31] [D loss: -200.020493] [G loss: 196.822052]\n",
      "[Epoch 173/200] [Batch 20/31] [D loss: -187.568268] [G loss: -50.883896]\n",
      "[Epoch 174/200] [Batch 0/31] [D loss: -170.963120] [G loss: 69.176964]\n",
      "[Epoch 174/200] [Batch 20/31] [D loss: -187.657089] [G loss: -104.249130]\n",
      "[Epoch 175/200] [Batch 0/31] [D loss: -155.216766] [G loss: 79.616165]\n",
      "[Epoch 175/200] [Batch 20/31] [D loss: -220.508362] [G loss: 221.050385]\n",
      "[Epoch 176/200] [Batch 0/31] [D loss: -187.882797] [G loss: -137.942902]\n",
      "[Epoch 176/200] [Batch 20/31] [D loss: -170.729904] [G loss: 142.608841]\n",
      "[Epoch 177/200] [Batch 0/31] [D loss: -185.430176] [G loss: 106.680374]\n",
      "[Epoch 177/200] [Batch 20/31] [D loss: -256.692169] [G loss: -238.214584]\n",
      "[Epoch 178/200] [Batch 0/31] [D loss: -188.168076] [G loss: 295.269989]\n",
      "[Epoch 178/200] [Batch 20/31] [D loss: -205.611267] [G loss: -271.163635]\n",
      "[Epoch 179/200] [Batch 0/31] [D loss: -199.145264] [G loss: 228.227585]\n",
      "[Epoch 179/200] [Batch 20/31] [D loss: -202.536957] [G loss: 105.901253]\n",
      "[Epoch 180/200] [Batch 0/31] [D loss: -166.901123] [G loss: 26.526226]\n",
      "[Epoch 180/200] [Batch 20/31] [D loss: -202.452454] [G loss: -49.760365]\n",
      "[Epoch 181/200] [Batch 0/31] [D loss: -139.950089] [G loss: 138.873215]\n",
      "[Epoch 181/200] [Batch 20/31] [D loss: -189.829941] [G loss: -87.552444]\n",
      "[Epoch 182/200] [Batch 0/31] [D loss: -134.806961] [G loss: 80.671204]\n",
      "[Epoch 182/200] [Batch 20/31] [D loss: -220.306274] [G loss: 150.235077]\n",
      "[Epoch 183/200] [Batch 0/31] [D loss: -186.747971] [G loss: -40.161537]\n",
      "[Epoch 183/200] [Batch 20/31] [D loss: -190.850861] [G loss: -15.137493]\n",
      "[Epoch 184/200] [Batch 0/31] [D loss: -153.495361] [G loss: 129.462921]\n",
      "[Epoch 184/200] [Batch 20/31] [D loss: -205.371628] [G loss: 110.334702]\n",
      "[Epoch 185/200] [Batch 0/31] [D loss: -160.644989] [G loss: 47.538776]\n",
      "[Epoch 185/200] [Batch 20/31] [D loss: -226.811569] [G loss: -40.842453]\n",
      "[Epoch 186/200] [Batch 0/31] [D loss: -178.218033] [G loss: 39.272392]\n",
      "[Epoch 186/200] [Batch 20/31] [D loss: -212.463440] [G loss: -140.216537]\n",
      "[Epoch 187/200] [Batch 0/31] [D loss: -195.861084] [G loss: 290.746826]\n",
      "[Epoch 187/200] [Batch 20/31] [D loss: -174.713898] [G loss: -62.985447]\n",
      "[Epoch 188/200] [Batch 0/31] [D loss: -159.643997] [G loss: -158.276489]\n",
      "[Epoch 188/200] [Batch 20/31] [D loss: -270.476624] [G loss: 308.428650]\n",
      "[Epoch 189/200] [Batch 0/31] [D loss: -187.887131] [G loss: -77.030762]\n",
      "[Epoch 189/200] [Batch 20/31] [D loss: -193.300354] [G loss: -45.959244]\n",
      "[Epoch 190/200] [Batch 0/31] [D loss: -129.858795] [G loss: -49.429245]\n",
      "[Epoch 190/200] [Batch 20/31] [D loss: -192.452530] [G loss: 241.899200]\n",
      "[Epoch 191/200] [Batch 0/31] [D loss: -102.892044] [G loss: -81.323029]\n",
      "[Epoch 191/200] [Batch 20/31] [D loss: -233.051788] [G loss: -81.631607]\n",
      "[Epoch 192/200] [Batch 0/31] [D loss: -123.228455] [G loss: 143.090942]\n",
      "[Epoch 192/200] [Batch 20/31] [D loss: -208.951736] [G loss: -141.440338]\n",
      "[Epoch 193/200] [Batch 0/31] [D loss: -165.998581] [G loss: 234.420883]\n",
      "[Epoch 193/200] [Batch 20/31] [D loss: -165.372620] [G loss: -105.017715]\n",
      "[Epoch 194/200] [Batch 0/31] [D loss: -114.320396] [G loss: 37.753899]\n",
      "[Epoch 194/200] [Batch 20/31] [D loss: -182.707184] [G loss: 82.992844]\n",
      "[Epoch 195/200] [Batch 0/31] [D loss: -145.877457] [G loss: 82.162514]\n",
      "[Epoch 195/200] [Batch 20/31] [D loss: -246.469971] [G loss: -175.824432]\n",
      "[Epoch 196/200] [Batch 0/31] [D loss: -169.285507] [G loss: 122.672890]\n",
      "[Epoch 196/200] [Batch 20/31] [D loss: -166.057281] [G loss: 47.293720]\n",
      "[Epoch 197/200] [Batch 0/31] [D loss: -184.666748] [G loss: -16.202499]\n",
      "[Epoch 197/200] [Batch 20/31] [D loss: -212.512482] [G loss: -59.256142]\n",
      "[Epoch 198/200] [Batch 0/31] [D loss: -188.135254] [G loss: 189.948761]\n",
      "[Epoch 198/200] [Batch 20/31] [D loss: -228.754913] [G loss: -38.267174]\n",
      "[Epoch 199/200] [Batch 0/31] [D loss: -150.007126] [G loss: -87.922165]\n",
      "[Epoch 199/200] [Batch 20/31] [D loss: -206.797714] [G loss: 172.441238]\n",
      "[Epoch 200/200] [Batch 0/31] [D loss: -155.076965] [G loss: -26.095648]\n",
      "[Epoch 200/200] [Batch 20/31] [D loss: -241.461594] [G loss: 179.925751]\n"
     ]
    }
   ],
   "source": [
    "batches_done = 0\n",
    "dataloader = dataloaders['train']\n",
    "EPOCHS = 200\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = imgs.type(Tensor)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Tensor(np.random.normal(0, 1, (imgs.shape[0], 100)))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        fake_imgs = generator(z).detach()\n",
    "        # Adversarial loss\n",
    "        loss_D = -torch.mean(discriminator(real_imgs)) + torch.mean(discriminator(fake_imgs))\n",
    "\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Clip weights of discriminator\n",
    "        for p in discriminator.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "        # Train the generator every n_critic iterations\n",
    "        if i % 20 == 0:\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Generate a batch of images\n",
    "            gen_imgs = generator(z)\n",
    "            # Adversarial loss\n",
    "            loss_G = -torch.mean(discriminator(gen_imgs))\n",
    "\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch+1, EPOCHS, batches_done % len(dataloader), len(dataloader), loss_D.item(), loss_G.item())\n",
    "            )\n",
    "\n",
    "        if batches_done % 400 == 0:\n",
    "            save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "        batches_done += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e328bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Tensor(np.random.normal(0, 1, (imgs.shape[0], 100)))\n",
    "gen_imgs = generator(z)\n",
    "gen_imgs = gen_imgs.cpu()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "num_to_gen = gen_imgs.shape[0]\n",
    "_, axes = plt.subplots(figsize=(16, 4), ncols=num_to_gen)\n",
    "for ii in range(num_to_gen):\n",
    "    ax = axes[ii]\n",
    "    img = gen_imgs[ii]\n",
    "    npimg = img.detach().numpy()\n",
    "    ax.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a9407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda4b4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
