{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae52c049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d/miniconda3/envs/tfg/lib/python3.7/site-packages/skimage/io/manage_plugins.py:23: UserWarning: Your installed pillow version is < 8.1.2. Several security issues (CVE-2021-27921, CVE-2021-25290, CVE-2021-25291, CVE-2021-25293, and more) have been fixed in pillow 8.1.2 or higher. We recommend to upgrade this library.\n",
      "  from .collection import imread_collection_wrapper\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from skimage.io import imread\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ae42bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# make sure to enable GPU acceleration!\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eab4560",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "dataset_folder_name = '../data/UTKFace/Images'\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.7\n",
    "IM_WIDTH = IM_HEIGHT = 200\n",
    "\n",
    "dataset_dict = {\n",
    "    'race_id': {\n",
    "        0: 'white', \n",
    "        1: 'black', \n",
    "        2: 'asian', \n",
    "        3: 'indian', \n",
    "        4: 'others'\n",
    "    },\n",
    "    'gender_id': {\n",
    "        0: 'male',\n",
    "        1: 'female'\n",
    "    }\n",
    "}\n",
    "\n",
    "dataset_dict['gender_alias'] = dict((g, i) for i, g in dataset_dict['gender_id'].items())\n",
    "dataset_dict['race_alias'] = dict((g, i) for i, g in dataset_dict['race_id'].items())\n",
    "\n",
    "\n",
    "def parse_dataset(dataset_path, ext='jpg'):\n",
    "    \"\"\"\n",
    "    Used to extract information about our dataset. It iterates over all images and return a DataFrame with\n",
    "    the data (age, gender and sex) of all files.\n",
    "    \"\"\"\n",
    "    def parse_info_from_file(path):\n",
    "        \"\"\"\n",
    "        Parse information from a single file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            filename = os.path.split(path)[1]\n",
    "            filename = os.path.splitext(filename)[0]\n",
    "            age, gender, race, _ = filename.split('_')\n",
    "\n",
    "            return int(age), dataset_dict['gender_id'][int(gender)], dataset_dict['race_id'][int(race)]\n",
    "        except Exception as ex:\n",
    "            return None, None, None\n",
    "\n",
    "    files = glob.glob(os.path.join(dataset_path, \"*.%s\" % ext))\n",
    "\n",
    "    records = []\n",
    "    for file in files:\n",
    "        info = parse_info_from_file(file)\n",
    "        records.append(info)\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df['file'] = files\n",
    "    df.columns = ['age', 'gender', 'race', 'file']\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    This is our custom dataset class which will load the images, perform transforms on them,\n",
    "    and load their corresponding labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.images = [os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith(\".jpg\")]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(f'#{idx}...', end='')\n",
    "        if idx >= self.df.shape[0]:\n",
    "            idx = self.df.shape[0]-1\n",
    "        img_path = self.df.iloc[idx]['file']\n",
    "#         print(\"img_path:\", img_path)\n",
    "        #print('OK')\n",
    "        img = imread(img_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        sample = {\n",
    "            \"image\": img,\n",
    "        }\n",
    "        sample[\"gender\"] = dataset_dict['gender_alias'][self.df.iloc[idx][\"gender\"]]\n",
    "#        sample[\"id\"] = self.df.loc[idx, \"id\"]\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        try:\n",
    "            return self.df.shape[0]\n",
    "        except AttributeError:\n",
    "            return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c66b636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_dataset(dataset_folder_name)\n",
    "\n",
    "train_indices, test_indices = train_test_split(df.index, test_size=0.25)\n",
    "\n",
    "transform_pipe = transforms.Compose([\n",
    "    transforms.ToPILImage(), # Convert np array to PILImage\n",
    "\n",
    "    # Resize image to 224 x 224 as required by most vision models\n",
    "    transforms.Resize(\n",
    "        size=(224, 224)\n",
    "    ),\n",
    "\n",
    "    # Convert PIL image to tensor with image values in [0, 1]\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "dataset = Dataset(\n",
    "    df=df,\n",
    "    img_dir=\"../data/UTKFace/Images/\",\n",
    "    transform=transform_pipe\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "463f034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training dataset loader will randomly sample from the train samples\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=torch.utils.data.SubsetRandomSampler(\n",
    "        train_indices\n",
    "    ),\n",
    "#     shuffle=True,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "# The testing dataset loader will randomly sample from the test samples\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    sampler=torch.utils.data.SubsetRandomSampler(\n",
    "        test_indices\n",
    "    ),\n",
    "#     shuffle=True,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"test\": test_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8a0e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance = dataloaders['train'].dataset[2]\n",
    "# img, gender = instance['image'], instance['gender']\n",
    "# img = img[0]\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.imshow(img)\n",
    "# plt.gray()\n",
    "# plt.show()\n",
    "# gender = dataset_dict['gender_id'][gender]\n",
    "# print(f'Gender: {gender}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "924ae164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 23510081 parameters\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet50()\n",
    "USE_GPU = True\n",
    "EPOCHS = 5\n",
    "\n",
    "# Replace final fully connected layer to suite problem\n",
    "model.fc = torch.nn.Sequential(\n",
    "    torch.nn.Linear(\n",
    "        in_features=2048,\n",
    "        out_features=1\n",
    "    ),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# Model training\n",
    "if device=='cuda':\n",
    "    model = model.cuda() # Should be called before instantiating optimizer\n",
    "    \n",
    "def count_parameters(model):\n",
    "    total = 0\n",
    "    for p in model.parameters():\n",
    "        layer_count = np.prod(p.shape)\n",
    "        total += layer_count\n",
    "        \n",
    "    return total\n",
    "\n",
    "print(f'The model has {count_parameters(model)} parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7008bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will save checkpoints to the checkpoints folder. Create it.\n",
    "!mkdir -p checkpoints\n",
    "def save_checkpoint(optimizer, model, epoch, filename):\n",
    "    checkpoint_dict = {\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'model': model.state_dict(),\n",
    "        'epoch': epoch\n",
    "    }\n",
    "    torch.save(checkpoint_dict, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(optimizer, model, filename):\n",
    "    checkpoint_dict = torch.load(filename)\n",
    "    epoch = checkpoint_dict['epoch']\n",
    "    model.load_state_dict(checkpoint_dict['model'])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "236ba957",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss() # Binary classification Male-Female\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "def train_for_epoch():\n",
    "    model.train()\n",
    "    \n",
    "    train_losses = []\n",
    "    samples = 0\n",
    "    loss_sum = 0\n",
    "    correct_sum = 0\n",
    "    for i, batch in enumerate(dataloaders['train']):\n",
    "        # move training data into GPU\n",
    "        X = batch[\"image\"]\n",
    "        genders = batch[\"gender\"]\n",
    "        if device=='cuda':\n",
    "            X = X.cuda()\n",
    "            genders = genders.cuda()\n",
    "        # clear previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward propagation\n",
    "        y = model(X)\n",
    "        # calculate loss for the batch\n",
    "        loss = criterion(y, genders.view(-1, 1).float())\n",
    "        # backpropagation to compute gradients\n",
    "        loss.backward()\n",
    "        # update model weights\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # We need to multiple by batch size as loss is the mean loss of the samples in the batch\n",
    "        loss_sum += loss.item() * X.shape[0]\n",
    "        samples += X.shape[0]\n",
    "        num_corrects = torch.sum((y >= 0.5).float() == genders.view(-1, 1).float())\n",
    "        correct_sum += num_corrects\n",
    "\n",
    "        # Print batch statistics every 50 batches\n",
    "        #if j % 50 == 49 and phase == \"train\":\n",
    "        print(f\"\\tB{i + 1} - loss: {float(loss_sum) / float(samples)}, acc: {float(correct_sum) / float(samples)}\")\n",
    "        \n",
    "    # calculate average training loss of the epoch\n",
    "    train_loss = np.mean(train_losses)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    \n",
    "    valid_losses = []\n",
    "    valid_set = []\n",
    "    y_pred = []\n",
    "    \n",
    "    # we don't need gradients for valiation (save memory)\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloaders['test']:\n",
    "            # move training data into GPU\n",
    "            X = batch[\"image\"].to(device)\n",
    "            genders = batch[\"gender\"].to(device)\n",
    "            valid_set.extend(genders)\n",
    "            \n",
    "            y = model(X)\n",
    "            loss = criterion(y, genders.view(-1, 1).float())\n",
    "            valid_losses.append(loss.item())\n",
    "            y_pred.extend(y.argmax(dim=1).cpu().numpy())\n",
    "            \n",
    "    valid_loss = np.mean(valid_losses)\n",
    "    \n",
    "    # collect predictions into y_pred and ground truth into y_true\n",
    "    y_pred = np.array(y_pred, dtype=np.float32)\n",
    "    y_true = np.array(valid_set, dtype=np.float32)\n",
    "    \n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    return valid_loss, accuracy\n",
    "\n",
    "\n",
    "def train(first_epoch, num_epochs):\n",
    "    train_losses, valid_losses = [], []\n",
    "    for epoch in range(first_epoch, first_epoch+num_epochs):\n",
    "        train_loss = train_for_epoch()\n",
    "        \n",
    "        valid_loss, valid_acc = validate()\n",
    "        \n",
    "        print(f'Epoch({epoch:03d}) train loss: {train_loss}'\n",
    "              f'val loss: {valid_loss}'\n",
    "              f'val acc: {valid_acc*100:.4f}%')\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "        # save a checkpoint\n",
    "        checkpoint_filename = f'checkpoints/utk-{epoch:03d}.pkl'\n",
    "        save_checkpoint(optimizer, model, epoch, checkpoint_filename)\n",
    "    \n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8a421d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tB1 - loss: 17.269392013549805, acc: 0.375\n"
     ]
    }
   ],
   "source": [
    "train_losses, valid_losses = train(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12544d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3b739e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bc92bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2a16c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8903077d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365b3541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfg] *",
   "language": "python",
   "name": "conda-env-tfg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
