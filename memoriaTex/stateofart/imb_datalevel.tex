El objetivo de estas técnicas es modificar las distribuciones de los datos para disminuir el grado de desbalanceo y/o reducir el ruido. Matemáticamente, se define el ``ratio de desbalanceo'' (IR) de un dataset \cite{johnson2019survey,aka2021measuring,nafi2020addressing} como:

\begin{equation}[EQ:IMBRAT]{Fórmula para el ratio de desbalanceo.}
	IR=\frac{tama\tilde{n}o_{clase\ grande}}{tama\tilde{n}o_{clase\ peque\tilde{n}a}} \geq 1
\end{equation}

El objetivo al aplicar estas técnicas es lograr un valor del IR cercano o igual a 1. Hecho esto, se entrena un modelo sobre la distribución modificada, comparando si su rendimiento mejora frente a un entrenamiento con la distribución original. Existen múltiples técnicas para lograr este fin, desde las más simples a otras más complejas.

Entre los métodos más sencillos, el submuestreo aleatorio (\textit{random undersampling}) descarta muestras al azar del grupo mayoritario; mientras que el sobremuestreo aleatorio (\textit{random oversampling}) duplica al azar muestras del grupo minoritario \cite{johnson2019survey}, pudiendo rotar o transponer imágenes, ampliar ciertas zonas o cambiar los colores a través de filtros de contraste. En \citet{lin2017clustering} se usa un método basado en \textit{clusters} de K-means para submuestrear la clase mayoritaria a los K centroides calculados, con K igual al número de muestras de la clase minoritaria. Por su simpleza, estos métodos son poco efectivos: el submuestreo descarta datos, reduciendo la cantidad de información de la que los modelos disponen para aprender \cite{johnson2019survey}; y el sobremuestreo tiende a producir sobreajuste (\textit{overfitting}), aún si se acompaña de cambios aleatorios en las copias (rotación, reflejo...), pues no aporta suficiente variedad \cite{arjovsky2017wasserstein,chawla2002smote}.

Por ello, en la bibliografía se han desarrollado métodos de sobremuestreo ``informados'' que refuercen las fronteras inter-clase, reduzcan el sobreajuste y mejoren la clasificación. Destaca el \textit{Synthetic Minority Oversampling Technique} (SMOTE) \cite{chawla2002smote}, una técnica que produce nuevas muestras minoritarias artificiales por medio de interpolar muestras existentes con sus vecinos cercanos. Algunas variantes, como \textit{borderline}-SMOTE y ADASYN \cite{nafi2020addressing}, tratan de mejorar la calidad del concepto original en las fronteras inter-clase, así como prestar más atención a las muestras más difíciles de aprender. Los estudios al respecto coinciden en que SMOTE es sustancialmente mejor que los métodos clásicos de sub y sobremuestreo \cite{chawla2002smote,nafi2020addressing,johnson2019survey}. Por ejemplo, en \citet{nafi2020addressing}, se consigue una F1-score del 62\% en SMOTE frente al 58\% del sobre/submuestreo.

No obstante, las técnicas como SMOTE y similares tienen un límite en la cantidad de variedad e ``información útil'' que pueden introducir en las muestras sintetizadas \cite{nafi2020addressing}. Es por ello que recientemente se han desarrollado modelos más complejos para aumentar la calidad de las muestras minoritarias, mejorando las métricas existentes hasta el momento \cite{johnson2019survey,siniosoglou2021unsupervised}. Destacan aquí la familia de las redes antagónicas generativas (GAN) \cite{arjovsky2017wasserstein,johnson2019survey,siniosoglou2021unsupervised,yan2019joint,GoogleGAN}. Una GAN es un modelo compuesto de dos redes neuronales: generador y discriminador. El objetivo del generador es crear artificialmente resultados que pretenden confundirse con datos reales. El discriminador trata de identificar cuándo una muestra es real o sintética. A medida que continúa el ciclo de entrenamiento entre las redes antagónicas, el generador comenzará a producir resultados ``más creíbles'', y el discriminador aprenderá a distinguirlos de los verídicos. Existen numerosas variantes de esta técnica, entre las que destacan la DCGAN \cite{nafi2020addressing}, que usa redes profundas para estabilizar el aprendizaje; y la WGAN \cite{arjovsky2017wasserstein}, que imita a la anterior pero introduce la función de pérdida \textit{Wasserstein-1} para aproximar mejor la distribución de los originales. En el mencionado artículo de \citet{nafi2020addressing}, la WGAN supera a todos los métodos anteriormente enunciados, alcanzando una F1-score del 65\%.