Expuesto el dominio de la clasificación de imágenes, se presentan los sesgos que pueden surgir en estos algoritmos. Ello permite introducir el sesgo por desbalanceo de clases, en el que se centra el resto de esta investigación.

El sesgo en el aprendizaje automático se produce cuando un algoritmo presenta resultados con un prejuicio sistemático debido a asunciones erróneas en el proceso de aprendizaje \cite{vluymans2019dealing}. La literatura estudiada \cite{johnson2019survey,aka2021measuring,kim2019multiaccuracy,krishnan2020understanding,li2019learning} identifica
% dos causas globales para la aparición del sesgo en la clasificación de imágenes: i) el sesgo en los conjuntos datos utilizados para el aprendizaje de los modelos (en el dominio del género facial, puede que ciertas etnias o incluso un género completo se halle infrarrepresentado), y ii) el sesgo introducido (o amplificado) por los algoritmos que usan esos modelos. Estas dos causas globales son las que se estudian en esta investigación, pero es importante recordar que, sea en los datos o sea en los algoritmos, el sesgo puede provenir de varias fuentes, como son las siguientes:
múltiples fuentes de sesgo posibles en los algoritmos de clasificación en imágenes, como las que se describen a continuación:

\begin{itemize}
    \fontsize{11pt}{12pt}\selectfont
    \item En primer lugar, el sesgo puede derivar de que los etiquetados de las muestras sean incorrectos. Los errores en el etiquetado pueden deberse a errores en etiquetadores automáticos (no supervisados), o al fallo de anotadores humanos. Por ejemplo, en \citet{kafkalias2022bias} se explora el sesgo introducido en las etiquetas en función del género y la raza del propio anotador, y se trata de cuantificarlo. Así, algunos estudios del estado del arte dan lugar a técnicas de ``etiquetado suave'' \cite{han2020toward} que defina automáticamente las fronteras entre clases; o diseñan modelos capaces de evaluar si unas u otras etiquetas se adecúan a cada muestra utilizando métodos de \textit{clustering}, como \citet{aka2021measuring}.

    \item En segundo lugar, está el sesgo producido por conjuntos de datos que no representan cuantitativamente la distribución de la realidad o que tienen una falta de representación para ciertos valores. Para nuestro dominio, si se trata de crear un algoritmo de clasificación que tenga en cuenta el género, deberá asegurarse una distribución paritaria entre las clases ``hombre'' y ``mujer'', o de lo contrario existirá una descompensación en el rendimiento del modelo al predecir la clase más numerosa frente a la otra. En consecuencia, el algoritmo aprende mejor las clases más representadas, pero se encuentra con dificultades a la hora de distinguir el resto. Se dice entonces que hay un ``desbalanceo'' entre las clases \cite{nafi2020addressing}, y es en este problema en el que se centra nuestra investigación. Las soluciones propuestas al desbalanceo van desde el aumento de datos de las clases minoritarias (bien con copias modificadas \cite{johnson2019survey}, bien con generación sintética de muestras \cite{GoogleGAN,arjovsky2017wasserstein,he2008adasyn,chawla2002smote}) hasta el refinado de los algoritmos para que presten especial atención a dichas clases infrarrepresentadas. Nos referiremos a estas familias de técnicas en las Secciones \ref{SEC:SOTA_DATALEVEL} y \ref{SEC:SOTA_ALGOLEVEL}, respectivamente.

    \item En tercer lugar, es fundamental tener presente que para lograr predicciones óptimas, no solo es necesario abordar el desbalanceo de clases, sino también considerar la representación adecuada de todos los atributos relevantes \cite{wu2020gender,vallimeena2019cnn,yilmaz2021evolutionary}. Por ejemplo, aunque un conjunto de datos pueda contener un número significativo de imágenes que representen tanto hombres como mujeres, será insuficiente si no se ha considerado adecuadamente la diversidad étnica de las personas. Según \citet{loo2018influence}, diferentes etnias pueden exhibir características distintivas de género, lo cual implica que un conjunto de datos compuesto predominantemente por individuos de origen caucásico no será eficaz al clasificar personas de otras razas.

    \item En cuarto lugar, hay que considerar que algunas técnicas de mitigación, como la generación sintética de muestras, pueden tener efectos adversos e incluso introducir sus propios sesgos \cite{yan2019joint}. Crear muestras que no existen en la realidad puede inducir fallos no intencionados en los sistemas de visión artificial. Por ejemplo, en \citet{rahimzadeh2020modified} se desaconseja la generación de imágenes de radiografías para distinguir pulmones enfermos de los sanos, ya que a fin de cuentas se estaría generando ``pacientes'' que no existen realmente. Esto se traduce en un sesgo entre las imágenes reales y las sintéticas \cite{yilma2021generation}. Estudios como \citet{yan2019joint} usan funciones de pérdida propias para influir el proceso de retropropagación, logrando mayor separación entre instancias de diversas clases; y más proximidad entre instancias dentro de la misma clase.

    \item Por último, otras fuentes secundarias de sesgo pueden ser el ruido presente en las propias imágenes \cite{li2019learning,tuncc2020fuzzy}; o el llamado ``sesgo contextual'' \cite{jiang2019robust}, en el que por ejemplo, un algoritmo entrenado exclusivamente con imágenes de hombres haciendo deporte tendría dificultades a la hora de detectar mujeres realizando las mismas actividades.

\end{itemize}


% lit. sesgo_paper: aka2021measuring, kim2019multiaccuracy, krishnan2020understanding, li2019learning
% lit. anotacion: han2020toward, aka2021measuring
% lit. desbalanceo: zhao2021lightweight, nafi2020addressing, zhong2021improving, han2020toward, zhang2020towards
% lit. gen. sintetica: yilma2021generation, yan2019joint

% kim2020fair -> comparacion de pre, in y post-processing para atacar el bias
% nafi2020addressing -> combatir sesgo datos: oversampling,undersampling,SMOTE,GAN